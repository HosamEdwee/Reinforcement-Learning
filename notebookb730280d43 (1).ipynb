{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction to Reinforcement Learning\n\nReinforcement Learning (RL) is an area of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward.\n\n## Key Concepts in RL\n\n- **Agent**: The learner or decision maker.\n- **Environment**: The world through which the agent moves.\n- **Action (A)**: All the possible moves that the agent can take.\n- **State (S)**: All the possible situations that the agent can be in.\n- **Reward (R)**: Feedback from the environment to assess the actions taken by the agent.\n- **Policy (Ï€)**: The strategy that the agent employs to determine the next action based on the current state.\n- **Value Function**: It predicts the expected reward for following a policy from a given state.\n- **Q-value or Action-Value Function**: It predicts the expected reward for taking a given action in a given state and following a policy thereafter.\n","metadata":{}},{"cell_type":"code","source":"# Pseudo-code to represent the RL framework interaction loop\n\nwhile not done:\n    action = agent.choose_action(state)\n    next_state, reward, done, info = environment.step(action)\n    agent.learn(state, action, reward, next_state)\n    state = next_state\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Q-learning update rule at time step \\( t \\) can be expressed as:\n\n\\[ Q_{\\text{new}}(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \\right] \\]\n\nWhere:\n\n- \\( Q(s, a) \\) is the current Q-value for state \\( s \\) and action \\( a \\).\n- \\( Q_{\\text{new}}(s, a) \\) is the new Q-value for state \\( s \\) and action \\( a \\).\n- \\( \\alpha \\) is the learning rate.\n- \\( r_{t+1} \\) is the reward received after taking action \\( a_t \\) in state \\( s_t \\).\n- \\( \\gamma \\) is the discount factor that weighs the importance of future rewards.\n- \\( \\max_{a} Q(s_{t+1}, a) \\) is the maximum Q-value for the next state \\( s_{t+1} \\) over all possible actions.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}